{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e38ba9",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e0e2f",
   "metadata": {},
   "source": [
    "1. Compute the Gradient of L explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a857b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-dimensional function\n",
    "theta_0 = 1.0\n",
    "\n",
    "def l(theta):\n",
    "    return (theta -3)**2 + 1\n",
    "\n",
    "def grad_l(theta):\n",
    "    return 2*(theta -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da98216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw L(theta)\n",
    "theta_vals = np.linspace(-1, 7, 100)\n",
    "L_vals = l(theta_vals)\n",
    "plt.plot(theta_vals, L_vals)\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('L(theta)')\n",
    "plt.title('Function L(theta)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79d1ae",
   "metadata": {},
   "source": [
    "2. Implement the Gradient Descent to optimize \n",
    " following what we introduced on the theoretical sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03583528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(l, grad_l, theta_0, maxit, eta, tolL, tolTheta):\n",
    "    loss_history = []\n",
    "    theta_history = []\n",
    "\n",
    "    # GD step\n",
    "    for k in range(maxit):\n",
    "        theta = theta_0 - eta * grad_l(theta_0)\n",
    "\n",
    "        # viene fatta la norma per poter avere una misura scalare del vettore (o se theta fosse una matrice della matrice)\n",
    "        # linalg.norm di base fa la norma 2 per i vettori e la norma di Frobenius per le matrici\n",
    "        # controlliamo che la lunghezza del gradiente non sia troppo corta \n",
    "        # e che la differenza tra i parametri attuali e quelli precedenti non sia troppo piccola\n",
    "        if(np.linalg.norm(grad_l(theta))<tolL or (np.linalg.norm(theta-theta_0)<tolTheta)):\n",
    "            break\n",
    "\n",
    "        loss_history.append(l(theta))\n",
    "        theta_history.append(theta_0)\n",
    "\n",
    "        theta_0 = theta\n",
    "    return theta, k, loss_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxit = 100\n",
    "tolL = 1e-6\n",
    "tolTheta = 1e-6\n",
    "eta = 0.1\n",
    "\n",
    "theta_opt, num_iter, _, _ = GD(l, grad_l, theta_0, maxit, eta, tolL, tolTheta)\n",
    "print(\"Optimal theta:\", theta_opt, \" iterations:\" , num_iter)\n",
    "print(\"Optimal value of L:\", l(theta_opt), \" value of L at theta0:\", l(theta_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd094d",
   "metadata": {},
   "source": [
    "3-4. Test three different constant step sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcade57",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = np.array([0.01, 0.2, 1.0])\n",
    "loss_histories = []\n",
    "theta_histories = []\n",
    "\n",
    "for eta in etas:\n",
    "    theta_0 = 1.0\n",
    "    theta_opt, num_iter, loss_history, theta_history = GD(l, grad_l, theta_0, maxit, eta, tolL, tolTheta)\n",
    "    loss_histories.append(loss_history)\n",
    "    theta_histories.append(theta_history)\n",
    "    print(f\"Eta: {eta} => Optimal theta: {theta_opt}, iterations: {num_iter}, L(theta_opt): {l(theta_opt)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting theta histories for different etas on the same graph with the x axis as iterations\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, eta in enumerate(etas):\n",
    "    plt.plot(theta_histories[i], label=f'eta={eta}')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Theta Value')\n",
    "plt.title('Theta History for Different Step Sizes')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss histories for different etas on the same graph with the x axis as iterations\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, eta in enumerate(etas):\n",
    "    plt.plot(loss_histories[i], label=f'eta={eta}')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss L(theta)')\n",
    "plt.title('Loss History for Different Step Sizes')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8aa91",
   "metadata": {},
   "source": [
    "Relate your observations to the discussion in class about:\n",
    "\n",
    "-step-size being too small / too large,\n",
    "-the role of convexity,\n",
    "-how the “just right” step size leads to fast convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaafe74",
   "metadata": {},
   "source": [
    "# Exercise 2: Backtracking Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d616c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(theta):\n",
    "    return theta**4 + 3*(theta**2) + 2\n",
    "\n",
    "def grad_l(theta):\n",
    "    return 4*(theta**3) + 6*theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a95f5",
   "metadata": {},
   "source": [
    "Implement Gradient Descent with Backtracking, using the Armijo condition, considering the backtracking(...) function from class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(L, grad_L, theta, eta0=1.0, beta=0.5, c=1e-4):\n",
    "    \"\"\"\n",
    "    Return a step size eta that satisfies the Armijo condition:\n",
    "        L(theta - eta*g) <= L(theta) - c * eta * ||g||^2\n",
    "    \"\"\"\n",
    "    eta = eta0\n",
    "    g = grad_L(theta)\n",
    "    g_norm2 = np.dot(g, g)\n",
    "\n",
    "    # se:   Loss_k+1 >= Loss_k - Armijo_constante * eta * norma_2_gradiente\n",
    "    # allora riduci eta\n",
    "    while L(theta - eta * g) > L(theta) - c * eta * g_norm2:\n",
    "        eta *= beta\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_backtracking(l, grad_l, theta_0, maxit, eta, tolL, tolTheta):\n",
    "    loss_history = []\n",
    "    theta_history = []\n",
    "\n",
    "    # GD step\n",
    "    for k in range(maxit):\n",
    "        # compute step size via backtracking \n",
    "        eta = backtracking(l, grad_l, theta_0, eta0=eta)\n",
    "        \n",
    "        theta = theta_0 - eta * grad_l(theta_0)\n",
    "\n",
    "        # viene fatta la norma per poter avere una misura scalare del vettore (o se theta fosse una matrice della matrice)\n",
    "        # linalg.norm di base fa la norma 2 per i vettori e la norma di Frobenius per le matrici\n",
    "        # controlliamo che la lunghezza del gradiente non sia troppo corta \n",
    "        # e che la differenza tra i parametri attuali e quelli precedenti non sia troppo piccola\n",
    "        if(np.linalg.norm(grad_l(theta))<tolL or (np.linalg.norm(theta-theta_0)<tolTheta)):\n",
    "            break\n",
    "\n",
    "        loss_history.append(l(theta))\n",
    "        theta_history.append(theta_0)\n",
    "\n",
    "        theta_0 = theta\n",
    "    return theta, k, loss_history, theta_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055bbf3e",
   "metadata": {},
   "source": [
    "2. Test different initial points theta=[-2 , 0.5, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b826c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_thetas = [-2.0, 0.5, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxit = 100\n",
    "tolL = 1e-6\n",
    "tolTheta = 1e-6\n",
    "eta = 0.5 # high initial step size because backtracking will reduce it as needed\n",
    "\n",
    "loss_histories = []\n",
    "theta_histories = []\n",
    "\n",
    "for theta_0 in starting_thetas:\n",
    "    theta_opt, num_iter, loss_history, theta_history = GD_backtracking(l, grad_l, theta_0, maxit, eta, tolL, tolTheta)\n",
    "    loss_histories.append(loss_history)\n",
    "    theta_histories.append(theta_history)\n",
    "    print(\"Starting theta:\", theta_0, \" Optimal theta:\", theta_opt, \" iterations:\" , num_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1a9c5",
   "metadata": {},
   "source": [
    "3. For each starting point, plot:\n",
    "    - the function curve L(theta) in 1D in the domain [-3, 3]\n",
    "    - the trajectory of the iterates theta overlaid on the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "for i, loss_history in enumerate(loss_histories):\n",
    "    # array con solo i valori in [-3, 3]\n",
    "    filter_loss_in_range = [loss for loss in loss_histories[i] if -3 <= loss <= 3]\n",
    "    plt.plot(filter_loss_in_range, label=f'Starting theta={starting_thetas[i]}')\n",
    "    plt.ylabel('Loss L(theta)')\n",
    "    plt.title('Loss History for Different Starting Thetas')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Starting theta 2 and -2 have the same loss history because of symmetry\n",
    "\n",
    "for i, hist in enumerate(loss_histories):\n",
    "    print(i, \"len:\", len(hist),\n",
    "          \"min:\", np.nanmin(hist) if len(hist)>0 else None,\n",
    "          \"max:\", np.nanmax(hist) if len(hist)>0 else None,\n",
    "          \"contains NaN:\", np.isnan(hist).any() if len(hist)>0 else None,\n",
    "          \"sample:\", hist[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b8b16",
   "metadata": {},
   "source": [
    "- Why different initializations converge to different minima.\n",
    "- How backtracking automatically chooses a suitable step size at each iteration.\n",
    "- Situations where constant step size would fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afc2dd",
   "metadata": {},
   "source": [
    "# Exercise 3: GD in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.0, 0.0],\n",
    "              [0.0, 25.0]])\n",
    "\n",
    "def l(theta):\n",
    "    return 1/2 * theta @ A @ theta.T \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869045a",
   "metadata": {},
   "source": [
    "How the gradient is computed:\n",
    "$$    L(x)= \\tfrac{1}{2}  x^{\\top} A x        $$\n",
    "$$x=\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix},\\qquad A=\\begin{pmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{pmatrix}$$\n",
    "$$L(x)=\\tfrac{1}{2}\\left(a_{11}x_1^2 + (a_{12}+a_{21})x_1x_2 + a_{22}x_2^2\\right).$$\n",
    "\n",
    "Siccome A è una matrice Simmetrica:\n",
    "$$a_{12} = a_{21} $$\n",
    "$$L(x)=\\tfrac{1}{2}\\left(a_{11}x_1^2 + 2a_{12}x_1x_2 + a_{22}x_2^2\\right).$$\n",
    "$$L(x)=\\tfrac{1}{2}a_{11}x_1^2 + a_{12}x_1x_2 + \\tfrac{1}{2}a_{22}x_2^2.$$\n",
    "\n",
    "Derivata parziale di x_1 \n",
    "$$\\frac{\\partial L}{\\partial x_1} = \\tfrac{1}{2}2a_{11}x_1 + a_{12}x_2 + 0 $$\n",
    "$$\\frac{\\partial L}{\\partial x_1} = a_{11}x_1 + a_{12}x_2$$\n",
    "\n",
    "Derivata parziale di x_2\n",
    "$$\\frac{\\partial L}{\\partial x_1} = 0 + a_{12}x_1 + \\tfrac{1}{2}2a_{22}x_2  $$\n",
    "$$\\frac{\\partial L}{\\partial x_2} = a_{21}x_1 + a_{22}x_2$$\n",
    "\n",
    "Inseriamo le derivate parziali come Righe:\n",
    "$$ ∇L(x)= \\begin{pmatrix} a_{11}x_1 & a_{12}x_2 \\\\a_{21}x_1 & a_{22}x_2\\end{pmatrix} $$\n",
    "$$ ∇L(x)= \\begin{pmatrix} a_{11} & a_{12} \\\\a_{21} & a_{22}\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l(theta):\n",
    "    return A @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1_vals = np.linspace(-3, 3, 100)\n",
    "theta2_vals = np.linspace(-3, 3, 100)\n",
    "Theta1, Theta2 = np.meshgrid(theta1_vals, theta2_vals)\n",
    "L_vals = 0.5 * (A[0,0]*Theta1**2 + A[1,1]*Theta2**2)\n",
    "\n",
    "# Plot tree-dimensional surface of L(theta)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(Theta1, Theta2, L_vals, cmap='viridis', alpha=0.8)\n",
    "ax.set_xlabel('Theta 1')\n",
    "ax.set_ylabel('Theta 2')\n",
    "ax.set_zlabel('L(theta)')\n",
    "ax.set_title('Surface of L(theta)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bc265",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = np.array([1.0, 1.0])\n",
    "\n",
    "maxit = 100\n",
    "tolL = 1e-6\n",
    "tolTheta = 1e-6\n",
    "etas = np.array([0.02, 0.05, 0.1])\n",
    "\n",
    "loss_histories = []\n",
    "theta_histories = []\n",
    "\n",
    "for eta in etas:\n",
    "    theta_0 = np.array([1.0, 1.0])\n",
    "    theta_opt, num_iter, loss_history, theta_history = GD(l, grad_l, theta_0, maxit, eta, tolL, tolTheta)\n",
    "    loss_histories.append(loss_history)\n",
    "    theta_histories.append(theta_history)\n",
    "    print(f\"Eta: {eta} => Optimal theta: {theta_opt}, iterations: {num_iter}, L(theta_opt): {l(theta_opt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_levelsets(A, xlim=(-3,3), ylim=(-3,3), ngrid=400, \n",
    "                   ncontours=12, title=None):\n",
    "    xs = np.linspace(xlim[0], xlim[1], ngrid)\n",
    "    ys = np.linspace(ylim[0], ylim[1], ngrid)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = 0.5*(A[0,0]*X**2 + 2*A[0,1]*X*Y + A[1,1]*Y**2)  # theta^T A theta, left-multiplied convention\n",
    "    cs = plt.contour(X, Y, Z, levels=ncontours)\n",
    "    plt.clabel(cs, inline=True, fontsize=8)\n",
    "    plt.axhline(0, lw=0.5, color='k')\n",
    "    plt.axvline(0, lw=0.5, color='k')\n",
    "    plt.gca().set_aspect('equal', 'box')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel(r'$\\theta_1$')\n",
    "    plt.ylabel(r'$\\theta_2$')\n",
    "    plt.grid(alpha=0.2)\n",
    "\n",
    "    # ensure axis bounds are set so subsequent plotting is clipped to these limits\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cf2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for theta_history in theta_histories:\n",
    "    x = np.array(theta_history)\n",
    "    quad_levelsets(A, xlim=(-1.2,1.2), ylim=(-1.2,1.2), title='Level Sets of L(theta)')\n",
    "    plt.plot(x[:,0], x[:,1], color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b010f",
   "metadata": {},
   "source": [
    "# Es 4 Exact Line Search vs Backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab327437",
   "metadata": {},
   "source": [
    "##  Ricerca Lineare Esatta (Exact Line Search)\n",
    "\n",
    "Quando un algoritmo di ottimizzazione (ad esempio, la Discesa del Gradiente) si trova in un punto $\\theta_k$ e ha calcolato una direzione di discesa $d_k$ (ad esempio, $d_k = -\\nabla L(\\theta_k)$), lo scopo della **Line Search** è risolvere il problema di ottimizzazione in una dimensione:\n",
    "\n",
    "$$\\eta^* = \\arg \\min_{\\eta > 0} L(\\theta_k + \\eta d_k)$$\n",
    "\n",
    "L'obiettivo è trovare il valore di $\\eta^*$ (il passo) che porta al **minimo assoluto** della funzione di perdita $L$ lungo la linea definita dalla direzione $d_k$ che parte da $\\theta_k$.\n",
    "\n",
    "###  Aspetto Teorico e Calcolo\n",
    "\n",
    "Il problema $L(\\theta_k + \\eta d_k)$ può essere visto come una nuova funzione di una sola variabile, $\\phi(\\eta)$:\n",
    "\n",
    "$$\\phi(\\eta) = L(\\theta_k + \\eta d_k)$$\n",
    "\n",
    "Per trovare il minimo $\\eta^*$, si applica il calcolo differenziale:\n",
    "\n",
    "1.  **Si calcola la derivata** di $\\phi(\\eta)$ rispetto a $\\eta$. Utilizzando la regola della catena, la derivata è:\n",
    "    $$\\frac{d\\phi}{d\\eta}(\\eta) = \\nabla L(\\theta_k + \\eta d_k)^T d_k$$\n",
    "\n",
    "2.  **Si imposta la derivata a zero** per trovare i punti stazionari:\n",
    "    $$\\nabla L(\\theta_k + \\eta d_k)^T d_k = 0$$\n",
    "\n",
    "L'equazione $g_{k+1}^T d_k = 0$ significa che il gradiente nel nuovo punto $\\theta_{k+1} = \\theta_k + \\eta d_k$ è **ortogonale** (perpendicolare) alla direzione di ricerca $d_k$.\n",
    "\n",
    "la **Ricerca Lineare Esatta (Exact Line Search)** richiede che il gradiente nel nuovo punto $\\theta_{k+1}$ sia ortogonale alla direzione di ricerca $d_k$:\n",
    "\n",
    "$$\\nabla L(\\theta_{k+1})^T d_k = 0$$\n",
    "\n",
    "Nella Discesa del Gradiente (GD), la direzione di ricerca $d_k$ è l'opposto del gradiente, quindi $d_k = -g_k$.\n",
    "\n",
    "## Come si Calcola Effettivamente $\\eta$ (Metodi Numerici)\n",
    "Nella pratica, l'equazione $\\nabla L(\\theta_{k} + \\eta d_k)^T d_k = 0$ è quasi sempre un'equazione non lineare in $\\eta$. Poiché non è possibile risolverla analiticamente (con carta e penna) per la maggior parte delle funzioni di perdita complesse (come quelle delle reti neurali), si ricorre a metodi di ricerca numerica unidimensionale. è molto costoso a livello computazionale perchè va fatta un ciclo iterativo per ogni step di discesa.\n",
    "$$\\eta_{j+1} = \\eta_j - \\frac{\\phi'(\\eta_j)}{\\phi''(\\eta_j)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62065e4",
   "metadata": {},
   "source": [
    "## Perchè in questo esercizio riusciamo a calolare eta senza cicli\n",
    "\n",
    "### Passo A: Sostituire il Gradiente\n",
    "\n",
    "Innanzitutto, calcoliamo il gradiente di $L(\\theta)$. Per una forma quadratica $L(\\theta)=\\frac{1}{2}\\theta^T A\\theta$, il gradiente è semplicemente:\n",
    "\n",
    "$$\\nabla L(\\theta) = A\\theta$$\n",
    "\n",
    "Quindi, il gradiente nel nuovo punto $\\theta_{k+1} = \\theta_k + \\eta_k d_k$ è:\n",
    "\n",
    "$$\\nabla L(\\theta_{k+1}) = A\\theta_{k+1} = A(\\theta_k + \\eta_k d_k)$$\n",
    "\n",
    "### Passo B: Applicare la Condizione di Ortogonalità\n",
    "\n",
    "Ora sostituiamo questa espressione e $d_k = -g_k$ nell'equazione di ortogonalità:\n",
    "\n",
    "$$(A(\\theta_k + \\eta_k d_k))^T d_k = 0$$\n",
    "\n",
    "Svolgiamo il prodotto scalare:\n",
    "\n",
    "$$(A\\theta_k)^T d_k + \\eta_k (Ad_k)^T d_k = 0$$\n",
    "\n",
    "### Passo C: Usare $g_k$ e Isoliamo $\\eta_k^{\\ast}$\n",
    "\n",
    "Ricordiamo che:\n",
    "\n",
    "* Il gradiente corrente è $g_k = A\\theta_k$.\n",
    "* La direzione di discesa è $d_k = -g_k$.\n",
    "\n",
    "Sostituendo $A\\theta_k = g_k$ e $d_k = -g_k$:\n",
    "\n",
    "$$(g_k)^T (-g_k) + \\eta_k (A(-g_k))^T (-g_k) = 0$$\n",
    "\n",
    "Semplifichiamo (tenendo presente che $g_k^T g_k = \\|g_k\\|^2$):\n",
    "\n",
    "$$-g_k^T g_k + \\eta_k ((-Ag_k)^T (-g_k)) = 0$$\n",
    "\n",
    "Poiché $(-Ag_k)^T (-g_k) = (Ag_k)^T g_k$ (il meno si annulla):\n",
    "\n",
    "$$-g_k^T g_k + \\eta_k (Ag_k)^T g_k = 0$$\n",
    "\n",
    "Riorganizziamo per isolare $\\eta_k^{\\ast}$:\n",
    "\n",
    "$$\\eta_k^{\\ast}(g_k^T A g_k) = g_k^T g_k$$\n",
    "\n",
    "Infine, troviamo la formula analitica per $\\eta_k^{\\ast}$:\n",
    "\n",
    "$$\\eta_k^{\\ast} = \\frac{g_k^T g_k}{g_k^T A g_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d22c455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m A = \u001b[43mnp\u001b[49m.array([[\u001b[32m5.0\u001b[39m, \u001b[32m0.0\u001b[39m],\n\u001b[32m      2\u001b[39m               [\u001b[32m0.0\u001b[39m, \u001b[32m2.0\u001b[39m]])\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34ml\u001b[39m(theta):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m/\u001b[32m2\u001b[39m * theta @ A @ theta.T\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "A = np.array([[5.0, 0.0],\n",
    "              [0.0, 2.0]])\n",
    "\n",
    "def l(theta):\n",
    "    return 1/2 * theta @ A @ theta.T\n",
    "\n",
    "def grad_l(theta):\n",
    "    return A @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a85ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_exact_line_search(l, grad_l, theta_0, maxit, tolL, tolTheta):\n",
    "    loss_history = []\n",
    "    theta_history = []\n",
    "\n",
    "    for k in range(maxit):\n",
    "        g = grad_l(theta_0)\n",
    "        # exact line search step size\n",
    "        eta = (g @ g) / (g @ A @ g)\n",
    "\n",
    "        theta = theta_0 - eta * g\n",
    "\n",
    "        if(np.linalg.norm(grad_l(theta))<tolL or (np.linalg.norm(theta-theta_0)<tolTheta)):\n",
    "            break\n",
    "\n",
    "        loss_history.append(l(theta))\n",
    "        theta_history.append(theta_0)\n",
    "\n",
    "        theta_0 = theta\n",
    "    return theta, k, loss_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(L, grad_L, theta, eta0=1.0, beta=0.5, c=1e-4):\n",
    "    eta = eta0\n",
    "    g = grad_L(theta)\n",
    "    g_norm2 = np.dot(g, g)\n",
    "\n",
    "    # se:   Loss_k+1 > Loss_k - Armijo_constante * eta * norma_2_gradiente\n",
    "    # allora riduci eta\n",
    "    while L(theta - eta * g) > L(theta) - c * eta * g_norm2:\n",
    "        eta *= beta\n",
    "    return eta\n",
    "\n",
    "def GD_backtracking(l, grad_l, theta_0, maxit, tolL, tolTheta, eta0=1.0):\n",
    "    loss_history = []\n",
    "    theta_history = []\n",
    "\n",
    "    # GD step\n",
    "    for k in range(maxit):\n",
    "        # compute step size via backtracking \n",
    "        eta = backtracking(l, grad_l, theta_0, eta0=eta0)\n",
    "        \n",
    "        theta = theta_0 - eta * grad_l(theta_0)\n",
    "\n",
    "        if(np.linalg.norm(grad_l(theta))<tolL or (np.linalg.norm(theta-theta_0)<tolTheta)):\n",
    "            break\n",
    "\n",
    "        loss_history.append(l(theta))\n",
    "        theta_history.append(theta_0)\n",
    "\n",
    "        theta_0 = theta\n",
    "    return theta, k, loss_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16255e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = np.array([3.0, 3.0])\n",
    "\n",
    "maxit = 100\n",
    "tolL = 1e-6\n",
    "tolTheta = 1e-6\n",
    "\n",
    "theta_opt_1, num_iter_1, loss_history_1, theta_history_1 = GD_exact_line_search(l, grad_l, theta_0.copy(), maxit, tolL, tolTheta)\n",
    "theta_opt_2, num_iter_2, loss_history_2, theta_history_2 = GD_backtracking(l, grad_l, theta_0.copy(), maxit, tolL, tolTheta)\n",
    "\n",
    "print(\"Exact Line Search: Optimal theta:\", theta_opt_1, \" iterations:\" , num_iter_1)\n",
    "print(\"Backtracking: Optimal theta:\", theta_opt_2, \" iterations:\" , num_iter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe201d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(theta_history_1)\n",
    "quad_levelsets(A, xlim=(-1.2,1.2), ylim=(-1.2,1.2), title='Exact Line Search')\n",
    "plt.plot(x[:,0], x[:,1], color='red')\n",
    "plt.show()\n",
    "\n",
    "x = np.array(theta_history_2)\n",
    "quad_levelsets(A, xlim=(-1.2,1.2), ylim=(-1.2,1.2), title='Backtracking')\n",
    "plt.plot(x[:,0], x[:,1], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(loss_history_1, label='Exact Line Search')\n",
    "plt.plot(loss_history_2, label='Backtracking')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Log(Loss L(theta))')\n",
    "plt.title('Loss History: Exact Line Search vs Backtracking')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMM2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
